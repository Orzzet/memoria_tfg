\chapter{Sistema inicial}\label{initial_system}

En este capítulo se va a describir el sistema inicial, introducido en el esquema general descrito anteriormente.

\section{Lenguaje y framework}\label{sec:language_and_framework}

\subsubsection{Lenguaje}\label{subsec:language}

El sistema se implementará completamente con el lenguaje de programación Python 3.7.

La elección del lenguaje Python es sencilla: los frameworks más utilizados de deep learning pueden ser usados en Python. Con una búsqueda rápida en GitHub bajo los términos ``deep learning'', ``machine learning''\phantom{1}y ``neural network''\phantom{1}se puede ver que los frameworks más populares pueden ser usados en Python.
\begin{itemize}
\item TensorFlow con 148k estrellas.
\item Keras con 49.4k estrellas.
\item PyTorch con 41.5k estrellas.
\item Caffe con 30.8k estrellas
\end{itemize}

Además, tal y como se muestra en el índice TIOBE \cite{Tiobe2020}, Python es el tercer lenguaje de programación con mejor puntuación teniendo en cuenta su presencia en los buscadores web.

Aún así Python es lento cuando se compara con lenguajes como C y esto se debe principalmente a 2 motivos:
\begin{enumerate}
\item Es interpretado, no compilado. Al compilar un programa escrito en un lenguaje compilado el compilador optimiza el programa con una multitud de técnicas, como el desenrollado de bucles, que elimina o reduce las instrucciones de control de un bucle. En Python el intérprete sólo accede a la instrucción a realizar, no realiza ninguna tarea de optimización.
\item Es de tipado dinámico. Esto quiere decir que el intérprete no sabe de qué tipo es una variable hasta que intenta acceder a ella, por lo que antes de acceder a su valor debe comprobar el tipo de variable. Gracias a esto la semántica del lenguaje puede omitir los tipos al declarar y asignar variables, pero el rendimiento es menor.
\end{enumerate}

Estas desventajas se pueden reducir gracias a que Python tiene la capacidad de llamar a subrutinas compiladas en C. Prácticamente todos los frameworks en los que se realizan operaciones con un alto coste computacional tienen la parte crítica de su código escrita en C.

En el ecosistema de Python, la base matemática para cualquier framework que haga operaciones matriciales es NumPy \cite{VanDerWalt2011}, ya que añade soporte para arrays de n dimensiones y multitud de operaciones de forma eficiente.

\subsubsection{Framework}\label{subsec:framework}

El framework utilizado será PyTorch 1.6 \cite{Paszke2019}.

En capítulos anteriores se mencionó el uso de fastai. Desde un principio se pretendía usar fastai para la parte de deep learning, pero en fastai no hay modelos que tomen como dato de entrada imágenes 3D. El framework permite implementar arquitecturas personalizadas para los modelos, pero para ello hay que utilizar PyTorch, ya que fastai está construido por encima de PyTorch. A causa de esto se decide aprender a usar PyTorch con un curso de Udacity \cite[Deep Learning with PyTorch]{Serrano2020}.

Una vez completado el curso, se decide seguir usando PyTorch en vez de fastai ya que al usar un software de más bajo nivel se tiene más control sobre las operaciones que se realizan.

Aún así, todos los frameworks populares de Python son una buena elección, ya que todos tienen \textit{bindings} a lenguajes de bajo nivel compilados, pero con la facilidad de uso de Python. Aunque TensorFlow puede ser difícil de utilizar, cuenta con Keras, que es un framework desarrollado por encima que abstrae muchas operaciones. PyTorch, basado en Torch \cite{Collobert2011}, también ofrece un uso fácil y buen rendimiento.

Un término muy usado en los frameworks de machine learning es el de \textit{tensor}. Un tensor se puede generalizar como una array de números dispuestos en una cuadrícula de dimensiones variables \cite[p. 31]{Goodfellow2016}. Será esta estructura de datos la usada para almacenar los datos necesarios involucrados en los cálculos de algoritmos de deep learning.

\section{Metas y m\'etricas}\label{sec:goals_and_metrics}

\cuadro{|c|c|c|}{Cuadro de métricas y metas.}{tab:metricas}{
Métrica & Buen resultado & Resultado ideal \\
\hline
IoU & $\geq 0.7$ & $\geq 0.9$ \\
}{Cuadro de métricas y metas}

Una buena métrica para la segmetación semántica es la utilizada en el artículo descrito en la sección \ref{sec:app1} \cite{Falk2019}, donde se usa la métrica intersección sobre unión (IoU) para comprobar los resultados.

\begin{equation}
M_{IoU}(A, B) := \frac{|A\cap B|}{|A\cup B|}
\end{equation}

Donde $A$ es el conjunto de vóxeles pertenecientes a la imagen perfectamente etiquetada y $B$ el conjunto de vóxeles pertenecientes a la predicción.

En ese mismo artículo se observa que si la tarea de etiquetado es realizada por un humano se obtiene un IoU $\sim0.9$ y se define como buen resultado un IoU $\sim0.7$.


\pagebreak \section{Arquitectura CNN}\label{sec:choose_cnn_arch}

Se ha implementado una arquitectura U-Net basada en la implementación de Ronneberger \cite{Ronneberger2015}. Se ha usado una implementación en PyTorch \cite{shiba242017} con la arquitectura mostrada en la figura \ref{fig:unetarch-half}. Se ha desarrollado una versión de la arquitectura UNet3D que se ha llamado MiniUNet3D, con menos capas. Esta versión se ha usado para comprobar que el primer sistema se ha implementado correctamente sin provocar errores en tiempo de ejecución.

\figura{0.8}{img/Diseno-unetarch-half}{Arquitectura U-Net reducida. Arquitectura completa (vista en \ref{sec:unet}) eliminando varias capas de convolución y una de pooling. Los modelos generados por esta arquitectura serán menos fiables, pero el entrenamiento será más rápido, por lo que es útil para implementar un primer sistema.}{fig:unetarch-half}{}{Arquitectura U-Net reducida.}

\section{Tamaño de la imagen en disco}\label{sec:disk_problem}

Al estar trabajando con herramientas en la nube será recomendable reducir el tamaño de los archivos lo mayor posible. Los archivos originales ocupan en total $9.17GB$, siendo el almacenamiento disponible de la máquina en la nube con la que se trabaja de $5GB$.

Al principio no se tuvo en cuenta la solución al tamaño de la imagen en memoria (sección \ref{sec:size_problem}) en la que se reduce la resolución de las imágenes. Esto habría resuelto también el problema del tamaño en disco. Aún así, en esta sección se aplican técnicas para reducir aún más el tamaño en disco sin perder información.

Todas las imágenes utilizan float de 8 bytes ($f8$) para almacenar los valores de cada vóxel. En el cuadro \ref{cuadro1:intensidad_min_max} se puede ver que el valor máximo de los vóxeles es $65535$, por lo que no es necesaria una precisión de 8 bytes ($[2^{-1023}, 2^{1023}]$) para los valores de esas imágenes, especialmente para el etiquetado, en el que se usa valores enteros entre $0$ y $100$.

\cuadro{|r|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}{Valores mínimo y máximo de los píxeles de las imágenes de entrada.}{cuadro1:intensidad_min_max}{
35.0 & 32.0 & 33.0 & 0.0 & 31.0 & 24.0 & 19.0 & 0.0 & 0.0 & 24.0 \\ 
4095.0 & 4095.0 & 4095.0 & 65535.0 & 4069.0 & 4095.0 & 4095.0 & 65535.0 & 65535.0 & 4095.0 \\
\hline
22.0 & 22.0 & 23.0 & 30.0 & 31.0 & 30.0 & 25.0 & 30.0 & 23.0 & 19.0 \\
4095.0 & 4095.0 & 4095.0 & 4095.0 & 4095.0 & 4095.0 & 4095.0 & 4095.0 & 4095.0 & 4095.0 \\
}{Valores mínimo y máximo de los píxeles de las imágenes de entrada.}

Si se hace que los valores de la imagen de entrada pasen de 8 bytes a 4 bytes, estaremos reduciendo su tamaño a la mitad. De forma similar si hacemos que la los valores de la imagen etiquetada pase de 8 bytes a 1 bytes, estaremos reduciendo su tamaño a una octava parte. Con 1 byte de precisión podremos almacenar hasta 256 valores distintos, suficiente ya que sólo tendremos 2 valores distintos: 0 para el fondo y 1 para la célula o los bordes. 

Además se comprimirán las imágenes sin pérdida, aunque esto sólo reducirá el tamaño del archivo, el tamaño de la imagen cargada en memoria será el mismo. Esta compresión será muy efectiva en las imágenes etiquetadas, ya que los elementos sólo tendrán 2 valores distintos. 

Después de hacer estos cambios se habrá reducido el tamaño del fichero pero no de las imágenes cargadas en memoria. Esto es así por dos motivos:
\begin{enumerate}
\item Las imágenes cargadas en memoria no tendrán ningún tipo de compresión. Cada valor del tensor ocupará el espacio correspondiente a su tipo de dato.
\item Las operaciones implementadas en los frameworks de deep learning limitan el tipo de dato que pueden tener los tensores. Están implementados a muy bajo nivel para optimizar las operaciones, por lo que será necesario  convertir los tipos a unos que acepten, si no lo son ya.
\end{enumerate}

Para leer los ficheros originales y guardarlos con el mismo formato se ha usado la librería  \textbf{h5py} \cite{Collette2013}. Ésta aporta una interfaz para trabajar con el formato de datos HDF5 \cite{HDFGroup20002010}, útil para usar en python ficheros generados en programas como MATLAB.

El tratamiento de datos con esta librería es muy intuitivo ya que imita la sintaxis de NumPy. Ha sido usado para archivos .mat generados en MATLAB con los datos iniciales. Tras el preprocesado, se ha usado esta librería para generar un nuevo archivo con todas las imágenes en un nuevo archivo .mat.

\begin{itemize}
\item \textbf{Tipo de dato.} Tanto la imagen de entrada como la imagen etiquetada tienen el tipo de datos $f8$. Es posible pasar la imagen de entrada al tipo $f4$ (real de 4 bits) y la imagen etiquetada al tipo $i1$ (entero de 1 bit) sin perder información. Esto va a reducir el tamaño en disco en más de la mitad. Las imágenes en el nuevo fichero se guardarán con los tipos $f4$ y $i1$.

\item \textbf{Compresión.} Se ha optado por usar la compresión \textit{gzip} \cite{Deutsch1996}, una compresión sin pérdida. Tiene 10 niveles de compresión $[0,9]$. Se ha probado a comprimir con el nivel $9$ consiguiendo una buena compresión pero es excesivamente lento, al final se ha optado por usar una compresión de nivel $4$ que, aunque tiene $\sim 10\%$ más tamaño que con el nivel $9$, el tiempo de la compresión se reduce a más de la mitad y de todas formas el resultado ya es muy bueno.
\end{itemize}

\section{Tamaño de la imagen en memoria}\label{sec:size_problem}

El principal cuello de botella al usar técnicas de deep learning es el hardware requerido para ello. 

Sería ideal ser capaz de entrenar una CNN usando las imágenes provistas directamente, pero a causa de su gran resolución no es viable. Una imagen de resolución $ (200, 1024, 1024)vx $ con una precisión de 8 bytes ocupa en memoria (siempre hablaremos de memoria de GPU) $size(imagen)=\frac{1024*1024*200*8}{1024*1024}=1600MB $. Si bien podríamos almacenar esta imagen en memoria, las CNN se caracterizan por aplicar un gran número de filtros distintos en paralelo a una imagen de entrada, utilizando los resultados de la aplicación de estos filtros en el siguiente paso de la CNN. Además, en el caso de U-Net, se guardan los resultados de algunas etapas para usarla en etapas futuras. Es completamente inviable usar la resolución original en este proyecto directamente como dato de entrada.

\subsubsection{Escalado hacia abajo}\label{subsubsec:downscale}

Las imágenes originales ocupan $\sim 2GB$ en memoria, intentar usarlas en una U-Net sobrepasa en gran medida los 16GB de memoria disponibles en este proyecto. Se ha decidido reducir las dimensiones en $(1/8, 1/8, 1/4)$ de sus dimensiones $(X, Y, Z)$ originales, consiguiendo una reducción de $8*8*4=256$ de su tamaño original, se pasa de $\sim 2GB$ a $\sim 8MB$. Con un tamaño de $8MB$ es posible realizar todo el proceso de entrenamiento e inferencia aunque se pierda calidad en la imagen. Gracias a la característica de U-Net de aceptar imágenes de cualquier dimensión, si no se hace entrenamiento por batch no es necesario asegurarse de que todas las imágenes tengan las mismas dimensiones.

\subsubsection{Escalado hacia arriba}\label{subsubsec:upscale}

Después de escalar hacia abajo, si se quiere hacer entrenamiento por batch con la imagen completa es necesario que todas las imágenes tengan las mismas dimensiones. Para esto se comprueba la mayor dimensión de las imágenes después de escalar hacia abajo y se elige como objetivo $(X_{obj}, Y_{obj}, Z_{obj})$. Para cada imagen $(X, Y, Z)$ el procedimiento es crear una añadir un padding de ceros de $X_{obj}-X$ a la derecha, de $Y_{obj}-Y$ hacia abajo y de $Z_{obj}-Z$ en profundidad. Al añadir padding en vez de interpolar no se deforman las imágenes.

El formato de las imágenes inicialmente es $ (Z,X,Y) $, siendo para todas las imágenes $ X = Y = 1024 $ y $ Z\epsilon[216,368] $. Además de reducir la resolución de las imágenes, es importante que todas tengan la misma, de lo contrario no podrán ser usadas en operaciones batch. También es esencial que la imagen de entrada y la imagen etiquetada no se deformen demasiado.

\subsubsection{Implementación}\label{subsec:size_problem_implementation}

Para implementar esta solución se ha utilizado la librería \textbf{SciPy} \cite{Virtanen2020}. 

Contiene diversos módulos con algoritmos útiles en el ámbito científico. En el submódulo \textit{ndimage} se pueden encontrar los algoritmos relativos al procesamiento de imagen. Se ha utilizado la función \textit{ndimage.zoom()} para reescalar las imágenes de entrada.

Esta función hace interpolación con splines, polinomios de orden n definidos a trozos. Se ha utilizado orden 3 porque es el orden por defecto y ha dado buenos resultados.

\section{Segmentación semántica multiinstancia con U-Net}\label{sec:multiinstance_segm}

El dataset disponible para este proyecto está compuesto por dos tipos distintos de imágenes. Uno es la imagen original y otro es una segmentación manual con el etiquetado perfecto. En el etiquetado perfecto cada célula está representada por un entero distinto sin espacio entre ellas, el fondo está representado por 0.

Si se usara el etiquetado actual sería necesario una clasificación con una clase por cada célula, pero eso no tiene mucho sentido ya que el número de células en una imagen puede variar y las células no tienen unas posiciones preestablecidas ni nada que las diferencien unas de otras (a priori, al menos). Que se sepa, todas las células tienen las mismas características por lo que deben tener la misma clase en una segmentación semántica. Tampoco se puede cambiar la etiqueta de todas las células a 1 (siendo 1 la clase célula) ya que las células están en contacto entre sí (no hay espacio entre células), lo que provocaría que los bordes originales de las células se perdieran.

La solución que se probará primero será añadir un espaciado entre células, tal y como se hace en el pipeline del artículo descrito en la sección \ref{sec:app1} \cite{Falk2019}. Al existir un espaciado entre células, éstas no estarían tocándose entre sí, por lo que todas podrían tener la misma etiqueta y contar como instancias distintas.

Para añadir este espaciado entre células primero se encuentran los bordes de cada célula y después se substraen de la imagen original. Por último se cambian todas las etiquetas a 1.

Para hacer estas operaciones se ha utilizado la librería \textbf{scikit-image} \cite{Walt2014}, una librería de procesamiento de imágenes.

\subsubsection{Encontrar bordes}\label{subsubsec:multi_find_borders}

Para encontrar los bordes se ha usado la función \textit{segmentation.find\_boundaries()}. Esta función realiza operaciones morfológicas para encontrar los bordes de los objetos de una imagen con valores enteros o binarios. Se han tomado las siguientes decisiones:
\begin{itemize}
\item Conectividad. Hay N tipos de conectividad siendo N el número de dimensiones de la imagen de entrada, en este caso N=3. Con conectividad 1 los vóxeles compartiendo al menos una cara son considerados vecinos. Con conectividad 2 los vóxeles compartiendo al menos un lado también son considerados vecinos. Con conectividad 3 los vóxeles compartiendo al menos una esquina también son considerados vecinos. Si dos vóxeles vecinos tienen distinta etiqueta, son bordes. Se ha elegido conectividad 3 para priorizar el que no haya células en contacto en ninguna de las 3 dimensiones.
\item Modo. Este modo define qué vóxeles son marcados como bordes de las componentes conexas. El modo escogido es \textit{outer}, que define como borde los vóxeles alrededor de las componentes conexas. Si hay 2 elementos en contacto se marca su frontera como borde. Se ha escogido este modo ya que es el que retiene la mayor cantidad de volumen celular original.
\end{itemize}

\subsubsection{Comprobar cantidad de células}\label{subsubsec:multi_count_cells}

También se ha usado la función (\textit{measure\_label()}) para comprobar que el nº de células no varía al hacer las operaciones de preprocesado (por ejemplo, podría ser que dos células con etiqueta 1 eliminaran el espacio entre sí, convirtiendose en la misma instancia de célula).

\section{Carga de datos}\label{sec:carga_datos}

Una vez todos los datos han sido preprocesados y almacenados en nuevos ficheros ya se pueden utilizar como fuente de datos para cargar las imágenes en tiempo de ejecución. En la carga de datos también se hace un tratamiento de imagen y será necesario que éste sea rápido, ya que se realizará cada vez que se acceda a una imagen durante el entrenamiento.

Para la carga de datos se han usado las siguientes librerías:
\begin{itemize}
\item \textbf{h5py.} La misma librería usada para crear los ficheros en la etapa anterior es usada para cargarlos.
\item \textbf{PyTorch.} PyTorch es el framework de deep learning elegido para este trabajo, por lo que se sacará el máximo provecho de lo que ofrezca de base. Para procesamiento de imagen es especialmente útil el paquete \textit{torchvision}.
\begin{itemize}
\item Clase base para Dataset: Para indicar dónde encontrar los datos y qué hacer con ellos, se usará la clase \textit{torchvision.Dataset} como base. Tan sólo hay que sobreescribir 3 métodos para que la clase sea funcional, estos son los métodos de inicialización (\textit{\_\_init\_\_}), para obtener la longitud del dataset (\textit{\_\_len\_\_}) y para obtener el siguiente ejemplo (\textit{\_\_getitem\_\_}), en el que se realizará un preprocesado y se devolverá la imagen de entrada y la imagen etiquetada.
\item DataLoader: Es usado para administrar el dataset. Se encarga de dar un nº de ejemplos aleatorios igual al batch seleccionado.
\end{itemize}
\end{itemize}

\section{Guardar modelo}\label{sec:store_model}

En cada iteración se ha guardado en un fichero información sobre el estado actual del entrenamiento y del modelo, con esto se puede utilizar para inferencia, para continuar el entrenamiento o para comprobar métricas. La información guardada es:

\begin{itemize}
\item \textbf{epochs:} Nº de epochs entrenados.
\item \textbf{best\_model\_state\_dict:} Pesos del modelo con menor error de validación.
\item \textbf{model\_state\_dict:} Pesos del modelo en la última iteración.
\item \textbf{optimizer\_state\_dict:} Pesos del optimizador en la última iteración. Al usar el optimizador Adam se guarda el \textit{learning rate} de cada parámetro de forma individual y este se va modificando a lo largo del aprendizaje, por lo que es beneficioso usar el último estado de estos pesos si se desea seguir con el entrenamiento.
\item \textbf{train\_losses:} Lista con todos los valores de pérdida en el entrenamiento.
\item \textbf{valid\_losses:} Lista con todos los valores de pérdida en la validación. train\_losses y valid\_losses son usados para dibujar las curvas de aprendizaje.
\item \textbf{test\_conf\_matrix:} Matriz de confusión por clases obtenida al evaluar el conjunto de test.
\item \textbf{test\_miou:} Media del IoU para el conjunto de test.
%\item \textbf{amp\_state\_dict:} Estado del optimizador usado con precisión mixta.
\end{itemize}

\section{Resultados}\label{sec:initial_system_resultados}

Para comprobar los resultados se han utilizado 3 herramientas: la gráfica de la función de pérdida durante el entrenamiento, la métrica intersección sobre unión (IoU) y una representación visual de la segmentación. Cuando se hable de de IoU se estará hablando de IoU celular, ya que es la métrica que indica cómo de bien se han etiquetado las células, siendo esta métrica calculada  siempre como la media de IoU de cada imagen del conjunto de test (2 imágenes)

\cuadro{|c|c|c|}{Métricas sistema inicial. En la primera columna se muestra cómo de bien se ha predicho el fondo. En la segunda columna, cómo de bien se han predicho las células.}{tab:metricas_inicial}{
IoU fondo & IoU células\\ 
\hline
0.9437 & 0.0018\\
}{Métricas sistema inicial.}

\figura{0.8}{img/pruebas/sistema_inicial_perdidas}{Pérdida de entrenamiento en cada iteración. 100 iteraciones. Se mide la pérdida media del conjunto de entrenamiento y el conjunto de validación.}{fig:sistema_inicial_perdidas}{}{Pérdida de entrenamiento y validación de sistema inicial.}

\figura{0.8}{img/pruebas/sistema_inicial_visual}{Ejemplo de segmentación del conjunto de test. Primera fila la imagen original, segunda fila segmentación objetiva, tercera fila segmentación predicha. Se muestra en cada columna Z=20, Z=25, Z=45, Z=50.}{fig:sistema_inicial_visual}{}{Segmentación de prueba de sistema inicial.}

En las métricas del cuadro \ref{tab:metricas_inicial} se muestra que el fondo está muy bien etiquetado, sin embargo las células están muy mal etiquetadas. Esto se puede ver visualmente en la figura \ref{fig:sistema_inicial_visual}. 

Sin embargo la gráfica de la función de pérdida (figura \ref{fig:sistema_inicial_perdidas}) muestra que el modelo está acercándose a su objetivo correctamente. Este error se verá corregido en el capítulo siguiente, efectuando la primera mejora según la metodología vista en la sección \ref{sec:metodologia} 