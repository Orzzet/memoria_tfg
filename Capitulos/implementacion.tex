\chapter{Implementación}\label{implementacion}

\section{Carga de datos}\label{sec:data_loading_processing}
\begin{itemize}
\item \textbf{TorchIO.} 

\end{itemize}
Además, después de la ZNormalization, se han normalizado los valore al rango $[0.0, 1.0]$.  

\subsection{Consideraciones}

\subsubsection{Tamaño del batch}

Se han hecho pruebas con varios tamaños de batch. El tamaño del batch determina cuántos ejemplos van a usarse en el entrenamiento del modelo. La entrada del modelo será de dimensión $(B,1,X,Y,Z)$, donde $B$ es el tamaño del batch. La ventaja de tener una entrada con este formato es se realizarán el mismo nº de operaciones matriciales sin importar el tamaño del batch. 
En el modelo MiniUnet3D al usar un batch de 1 los epochs tardan 11 segundos y al usar un batch de 4 tardan 7 segundos. Por cada batch utilizado se necesitan $\sim 3GB$ de memoria, por lo que se tomó como una buena opción para mejorar la velocidad de entrenamiento, aunque los resultados de este modelo eran malos ya que es una arquitectura reducida Unet3D.
Al entrenar con el modelo Unet3D, se comprobó que el nº de batch máximo posible era 2 debido a que se requiere más memoria al haber más capas. Con un batch de 1 tarda 39 segundos por epoch, con un batch de 2 tarde 34 segundos. Sin embargo, los resultados para un batch de 1 eran mejores, por lo que al se optó por usar un batch de 1, provocando que el tamaño de las imágenes usadas en el entrenamiento no tenga por qué ser el mismo.


\section{U-Net}\label{sec:unet_implementation}

\subsubsection{Funciones de pérdida}
Se han probado 3 funciones de pérdida: la entropía cruzada, la entropía cruzada con pesos y la función de pérdida DICE.

La entropía cruzada se descartó rápidamente ya que, al ser las dos clases muy desequilibradas daba demasiada prioridad al fondo sobre la segmentación y se tendía a perder mucha segmentación. 

Tras esto se probó la pérdida DICE. Para ello se utilizó la función \textit{Softmax} en la salida del modelo (logits) para que la suma de las probabildades de que un vóxel pertenezca al fondo y a la segmentación sume 1. El resultado y el etiquetado perfecto se pasaron al formato \textit{one hot} y se compararon con la función de pérdida DICE. Esto dio muy buenos resultados, consiguiendo por primera vez un $IoU>0.7$ para la segmentación.

Por último se probó la entropía cruzada con pesos, dando como peso a la clase de segmentación $1$ y se probó darle a la clase borde $0$, $0.1$ y $0.2$, sin obtener buenos resultados en ningún caso. En el artículo estudiado en los antecedentes \cite{Falk2019} se usó entropía cruzada con pesos dando dando pesos distintos a cada vóxel. No se llegó a probar esta implementación, que podría haber sido buena para la segmentación con espaciado entre células.

\subsubsection{Precisión Mixta}


